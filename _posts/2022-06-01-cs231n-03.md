---
title: "[CS231N] 4. Intruduction to Neural Networks"
categories:
  - cs231n
  - Image Classification
tags:
  - cs231
  - image classification
toc: true
toc_sticky: true
toc_label: "On This Page"
toc_icon: "bookmark"
use_math: true
---


ğŸ’¡ Iamge Classification <br>
Stanford University "CS231N" ê°•ì˜ë¥¼ ë“£ê³  ì •ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤. <br>
{: .notice--info}


ìë§‰ : https://github.com/visionNoob/CS231N_17_KOR_SUB (í¬ë¡¬ í™•ì¥ í”„ë¡œê·¸ë¨ Subtitles for Youtube) <br>
2017 ë²„ì „ ê°•ì˜ ëª©ì°¨ ë° ìŠ¬ë¼ì´ë“œ : http://cs231n.stanford.edu/2017/syllabus.html <br>
2017 ë²„ì „ ê°•ì˜ ë™ì˜ìƒ ë§í¬ : https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv <br>
2022 ë²„ì „ ê°•ì˜ ëª©ì°¨ ë° ìŠ¬ë¼ì´ë“œ : http://cs231n.stanford.edu/schedule.html <br>
ì‹¤ìŠµ ê³¼ì œ : https://github.com/cs231n/cs231n.github.io/tree/master/assignments <br>
{: .notice}

# Lecture 4 : Intruduction to Neural Networks

## Analytic gradient (using Computational graphs)

![L4_01](https://user-images.githubusercontent.com/86525868/171412748-514cbc3a-3ae5-4004-86ae-242278eead2b.png){: .align-center}

**inputì´ $x, W$ì¸ Linear Classifier**

* íŒŒë¼ë¯¸í„° Wì™€ ë°ì´í„° xì˜ ê³±ì…ˆì€ score vectorë¥¼ ì¶œë ¥í•¨ 

* hinge lossë¼ëŠ” ë‹¤ë¥¸ ê³„ì‚° ë…¸ë“œë¥¼ ê°€ì§€ê³  ìˆê³  ë°ì´í„° í•­$ L_i$ë¥¼ ê³„ì‚°í•˜ëŠ”ë°, ì˜¤ë¥¸ìª½ í•˜ë‹¨ì— ë³´ì´ëŠ” Regularization ë˜í•œ ê°€ì§€ê³  ìˆìŒ

* ì´ ë…¸ë“œ(R)ëŠ” regularizationí•­ì„ ê³„ì‚°í•˜ê³  ìµœì¢… loss, Lì€ regularization í•­ê³¼ ë°ì´í„° í•­ì˜ í•©ì„

* computational graphë¥¼ ì‚¬ìš©í•´ì„œ í•¨ìˆ˜ë¥¼ í‘œí˜„í•˜ê²Œ ë¨ìœ¼ë¡œì¨ back propagationì„ ì‚¬ìš©í• ìˆ˜ ìˆê²Œ ë¨

  * back propagationì€ gradientë¥¼ ì–»ê¸° ìœ„í•´ computataional graph ë‚´ë¶€ì˜ ëª¨ë“  ë³€ìˆ˜ì— ëŒ€í•´ chain ruleì„ ì¬ê·€ì ìœ¼ë¡œ ì‚¬ìš© 

    

![L4_02](https://user-images.githubusercontent.com/86525868/171412761-d62d56e9-df79-42c1-a374-8d2fdc32dd6a.png){: width="50%" height="50%"}{: .align-center}

ìˆ˜ì—…ì—ì„œ ì‚¬ìš©í•  CNNì˜ ê°€ì¥ ìœ„ì¸µì— ì…ë ¥ ì´ë¯¸ì§€ê°€ ë“¤ì–´ê°€ê³  ì•„ë˜ì—ëŠ” lossê°€ ìˆìŒ â†’ ì…ë ¥ ì´ë¯¸ì§€ëŠ” loss functionìœ¼ë¡œ ê°€ê¸°ê¹Œì§€ ë§ì€ layerë¥¼ ê±°ì³ ë³€í˜•ì„ ê²ªê²Œ ë¨



## Backpropagation : a simple example

$$
f(x, y, z)=(x+y)z
$$

ìš°ì„  ëª©í‘œëŠ” í•¨ìˆ˜ë¥¼ ê°€ì§€ëŠ” ê²ƒ (ì´ ê²½ìš°ì˜ í•¨ìˆ˜ëŠ” fë¡œ ì •ì˜í•¨) â†’ ê·¸ë¦¬ê³  function $f$ì˜ ì¶œë ¥ì— ëŒ€í•œ ì–´ë–¤ ë³€ìˆ˜ì˜ gradientë¥¼ ì°¾ê¸° ì›í•¨

1. í•­ìƒ í•¨ìˆ˜ $f$ë¥¼ ì´ìš©í•´ì„œ computational graphë¡œ ë‚˜íƒ€ëƒ„ 

    í•¨ìˆ˜ë¥¼ graphë¡œ ë‚˜íƒ€ë‚´ê³  ì´ ë„¤íŠ¸ì›Œí¬ì— ìš°ë¦¬ê°€ ê°€ì§€ê³  ìˆëŠ” ê°’$(x=-2, y=5, z=-4)$ë¥¼ ì „ë‹¬ <br>

    ![L4_03](https://user-images.githubusercontent.com/86525868/171412767-fc5767cd-4d76-42bd-bb7c-f68652ddce89.png){: width="60%" height="60%"}{: .align-center}

    $x+y$ ë§ì…ˆ ë…¸ë“œì˜ ì´ë¦„ì€ $q$ì´ê³ , $x$ì™€ $y$ì— ê°ê°ì— ëŒ€í•œ $q$ì˜ gradientëŠ” ë‹¨ìˆœ ë§ì…ˆì´ê¸° ë•Œë¬¸ì— ê°’ì€ 1

    $q$ì™€ $z$ì— ëŒ€í•œ $f$ ì˜ gradientëŠ” ê³±ì…ˆ ê·œì¹™ì— ì˜í•´ ê°ê° $z$ì™€ $q$ â†’ __ì°¾ê³ ì í•˜ëŠ”ê²ƒì€ $x, y, z$ ê°ê°ì— ëŒ€í•œ $f$ì˜ gradient__ <br>
    
### Another Example 

![L4_11](https://user-images.githubusercontent.com/86525868/171656454-49e709c8-8eee-481e-82bf-cae6291d57f8.png){: .align-center}
