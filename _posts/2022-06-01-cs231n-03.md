---
title: "[CS231N] 4. Intruduction to Neural Networks"
categories:
  - cs231n
  - Image Classification
tags:
  - cs231
  - image classification
toc: true
toc_sticky: true
toc_label: "On This Page"
toc_icon: "bookmark"
use_math: true
---


💡 Iamge Classification <br>
Stanford University "CS231N" 강의를 듣고 정리하였습니다. <br>
{: .notice--info}


자막 : https://github.com/visionNoob/CS231N_17_KOR_SUB (크롬 확장 프로그램 Subtitles for Youtube) <br>
2017 버전 강의 목차 및 슬라이드 : http://cs231n.stanford.edu/2017/syllabus.html <br>
2017 버전 강의 동영상 링크 : https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv <br>
2022 버전 강의 목차 및 슬라이드 : http://cs231n.stanford.edu/schedule.html <br>
실습 과제 : https://github.com/cs231n/cs231n.github.io/tree/master/assignments <br>
{: .notice}

# Lecture 4 : Intruduction to Neural Networks

## Analytic gradient (using Computational graphs)

![L4_01](https://user-images.githubusercontent.com/86525868/171412748-514cbc3a-3ae5-4004-86ae-242278eead2b.png){: .align-center}

**input이 $x, W$인 Linear Classifier**

* 파라미터 W와 데이터 x의 곱셈은 score vector를 출력함 

* hinge loss라는 다른 계산 노드를 가지고 있고 데이터 항$ L_i$를 계산하는데, 오른쪽 하단에 보이는 Regularization 또한 가지고 있음

* 이 노드(R)는 regularization항을 계산하고 최종 loss, L은 regularization 항과 데이터 항의 합임

* computational graph를 사용해서 함수를 표현하게 됨으로써 back propagation을 사용할수 있게 됨

  * back propagation은 gradient를 얻기 위해 computataional graph 내부의 모든 변수에 대해 chain rule을 재귀적으로 사용 

    

![L4_02](https://user-images.githubusercontent.com/86525868/171412761-d62d56e9-df79-42c1-a374-8d2fdc32dd6a.png){: width="50%" height="50%"}{: .align-center}

수업에서 사용할 CNN의 가장 위층에 입력 이미지가 들어가고 아래에는 loss가 있음 → 입력 이미지는 loss function으로 가기까지 많은 layer를 거쳐 변형을 겪게 됨



## Backpropagation : a simple example

$$
f(x, y, z)=(x+y)z
$$

우선 목표는 함수를 가지는 것 (이 경우의 함수는 f로 정의함) → 그리고 function $f$의 출력에 대한 어떤 변수의 gradient를 찾기 원함

1. 항상 함수 $f$를 이용해서 computational graph로 나타냄 

    함수를 graph로 나타내고 이 네트워크에 우리가 가지고 있는 값$(x=-2, y=5, z=-4)$를 전달 <br>

    ![L4_03](https://user-images.githubusercontent.com/86525868/171412767-fc5767cd-4d76-42bd-bb7c-f68652ddce89.png){: width="60%" height="60%"}{: .align-center}

    $x+y$ 덧셈 노드의 이름은 $q$이고, $x$와 $y$에 각각에 대한 $q$의 gradient는 단순 덧셈이기 때문에 값은 1

    $q$와 $z$에 대한 $f$ 의 gradient는 곱셈 규칙에 의해 각각 $z$와 $q$ → __찾고자 하는것은 $x, y, z$ 각각에 대한 $f$의 gradient__ <br>
    
### Another Example 

![L4_11](https://user-images.githubusercontent.com/86525868/171656454-49e709c8-8eee-481e-82bf-cae6291d57f8.png){: .align-center}
