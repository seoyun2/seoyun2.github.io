---
title: "[CS231N] 2. Image Classification"
categories:
  - cs231n
  - Image Classification
tags:
  - cs231
  - image classification
toc: true
toc_sticky: true
toc_label: "On This Page"
toc_icon: "bookmark"
use_math: true
---


💡 Iamge Classification <br>
Stanford University "CS231N" 강의를 듣고 정리하였습니다. <br>
{: .notice--info}

자막 : https://github.com/visionNoob/CS231N_17_KOR_SUB (크롬 확장 프로그램 Subtitles for Youtube) <br>
2017 버전 강의 목차 및 슬라이드 : http://cs231n.stanford.edu/2017/syllabus.html <br>
2017 버전 강의 동영상 링크 : https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv <br>
2022 버전 강의 목차 및 슬라이드 : http://cs231n.stanford.edu/schedule.html <br>
실습 과제 : https://github.com/cs231n/cs231n.github.io/tree/master/assignments <br>

# 2. Image Classification

<img width="946" alt="L2_01" src="https://user-images.githubusercontent.com/86525868/170157401-40c15f2e-87e4-489b-ba63-2059dd9c8f91.png">{: width="50%" height="50%"}{: .align-center}

컴퓨터에게 고양이 사진은 큰 격자 모양의 숫자집합으로 인식이 되고 이 거대한 숫자집합에서 **고양이**를 인식하는 것은 어려운 일

사람들은 이 이미지를 **"고양이"**라는 레이블로 인식하고 컴퓨터는 거대한 숫자 집단으로 인식하는 것을 **Semantic Gap(의미론적 차이)**이라고 함

<img width="1240" alt="L2_02" src="https://user-images.githubusercontent.com/86525868/170157543-d507a044-0fd6-4904-8ba4-50f56e4c2709.png">{: .align-center}

하지만 같은 고양이도 각도와 빛, 자세가 달라지면 픽셀들은 모두 달라지고 배경과 비슷한 색의 고양이와 다양한 고양이 종도 모두 고양이라고 인식하는 것은 어려울 수 있음 → **Image Classification 알고리즘은 이런 변수에 강해야 함** <br>



## Data-Driven Approach 

이미지 분류를 가능하게 하는 하나의 Insight로 Data-Driven Approach(데이터 중심 접근 방법)이 있음

고양이는 무엇이다, 물고기는 무엇이다라고 직접 규칙을 써내려가는 것 대신에 Google Image Serach와 같은 도구로 많은 데이터를 수집하고 이 데이터 셋을 이용해 Machine Learning Classifier를 학습시킬 수 있음

ML Algorithm은 데이터를 요약해서 다양한 객체들을 인식할 수 있는 모델을 만들고 학습 모델로 새로운 이미지를 테스트하면 사물을 인식할 수 있음 → Machine Learning의 Key Insight인 **두 개의 함수 필요**

1. Train Function → input = image, label / output = model 
2. Predict Function → input = model / output = predict <br>




## First Classifier : Nearest Neighbor

1. Memorize all data and labels

   ```python
   def train(images, labels):
   	# Machine Learning
     return model
   ```

2. Predict the label of the most similar training image

   ```python
   def predict(model, test_images):
   	# Use model to predict labels
   	return test_labels
   ```

간단한 모델이지만 Data-Driven Approach를 이해하기 좋은 Algorithm <br>



### CIFAR-10

총 50,000여개의 학습용 이미지가 각 10개의 카테고리에 균일하게 분포되어 있고 10,000여개의 테스트 이미지로 이루어진 데이터 셋 

<img width="998" alt="L2_03" src="https://user-images.githubusercontent.com/86525868/170157675-16886d7a-143c-4b43-a1d1-29d0c3a4e5cb.png">{: width="80%" height="80%"}{: .align-center}


오른쪽 칸의 맨 왼쪽 열은 CIFAR-10의 테스트 이미지이고 오른쪽 방향으로 학습 이미지 중 테스트 이미지와 유사한 순으로 정렬 되어 있음

NN Algorithm을 적용시키면 트레이닝 셋에서 **가장 가까운 샘플**을 찾게되고 **가장 가까운 샘플**의 레이블로 테스트 이미지의 레이블을 알 수 있음

❗️중요한 점은 test image와 train image를 어떻게 비교하는가 = **어떤 비교 함수를 사용할 것인가**❗️ <br>



### Distance Metric to compare images : L1 distance 

$$
d_1(I_1, I_2)=\sum_p |I_1^p-I_2^p|
$$

<img width="1134" alt="L2_04" src="https://user-images.githubusercontent.com/86525868/170157685-537d9704-a0e3-4c83-8b2b-57143683dc09.png">{: width="50%" height="50%"}{: .align-center}


L1 distance는 이미지를 Pixel-Wise로 비교 → **두 이미지간의 차이를 어떻게 측정할 것인가에 구체적인 방법을 제시**

```python
import numpy as np

class NearestNeighbor(object):
  def __init__(self):
    pass

  def train(self, X, y):
    """ X is N x D where each row is an example. Y is 1-dimension of size N """
    # the nearest neighbor classifier simply remembers all the training data
    self.Xtr = X
    self.ytr = y

  def predict(self, X):
    """ X is N x D where each row is an example we wish to predict label for """
    num_test = X.shape[0]
    # lets make sure that the output type matches the input type
    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)

    # loop over all test rows
    for i in range(num_test):
      # find the nearest training image to the i'th test image
      # using the L1 distance (sum of absolute value differences)
      distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)
      min_index = np.argmin(distances) # get the index with smallest distance
      Ypred[i] = self.ytr[min_index] # predict the label of the nearest example

    return Ypred
```

Train Function에서는 학습 데이터를 기억하고 Test Function에서는 이미지를 입력으로 받고 L1 Distance로 비교하며 학습 데이터 중에서 테스트 이미지와 가장 유사한 이미지를 찾아냄 

**Q1. With NN examples, how fast are training and prediction?**

* Train time은 데이터를 기억하기 때문에 상수시간이 걸리지만 Test time은 N개의 학습 데이터를 전부 테스트 이미지와 비교하기 때문에 상당한 시간이 소요됨 → **우리는 반대의 상황을 기대 (Classifier의 좋은 성능을 위해 train time에 투자 하는 방향으로 기대)** <br>


### K-NN Algorithm

**NN's Decision Regions**

각 점은 학습데이터이고 점의 색은 카테고리를 뜻하고 모든 좌표에서 각 좌표가 어떤 학습 데이터와 가장 가까운지 계산하고 각 좌표를 해당 카테고리로 칠함

NN Classifier은 공간을 나눠서 각 카테고리로 분류함

<img width="1159" alt="L2_05" src="https://user-images.githubusercontent.com/86525868/170157708-67bb7a7d-28b5-4699-87ee-5c6b0aa4486e.png">{: width="80%" height="80%"}{: .align-center}


* **K = 1** : NN Algorithm, 가장 가까운 이웃만을 보기 때문에 녹색 영역 중간에 노란색 영역이 생기고 파란색 영역에 녹색이 침범 (NN Classifier에서 발생 가능한 문제점) → **noise와 spurious**

이런 문제를 해결하기 위해 일반화된 버전인 **K-NN Algorithm**이 탄생 → **Distance metric을 이용하여 가까운 이웃을 K개 만큼 찾고, 이웃끼리 투표하는 방법**

* **K = 3** : 녹색 영역이 깔끔해지고 빨강, 파랑 영역의 경계들도 부드러워짐 
* **K = 5** : 각 영역들의 경계가 더 부드러워짐 <br>


**K-Nearest Neighbors : Distance Metric**

![L2_06](https://user-images.githubusercontent.com/86525868/170157732-2193baa2-a8f1-4d6f-a252-caecf34999bc.jpeg){: .align-center}


지금까지 L1 Distance 만 고려했지만 L2 Distance도 사용할 수 있음 → **공간의 근본적인 기하학적 구조가 다름**

좌표계를 회전시킨다면 L1 Distance는 변하지만 L2 Distance는 변하지 않음 → **L1은 좌표축의 영향을 받지만, L2는 고르게 영향을 받음**

* **L1 Distance**
  * 특징 벡터의 각각 요소들이 개별적인 의미를 가지고 있는 경우 → **특정 벡터의 영향이 강하게 적용됨**
  * 좌표 시스템의 영향을 받고 결정 경계를 만듦 → 부자연스러움
* **L2 Distance** 
  * 특징 벡터가 일반적인 벡터이고 요소들간 실질적인 의미를 잘 모르는 경우 → **특정 벡터의 영향이 고르게 적용됨**
  * 좌표 시스템의 영향을 받지 않고 결정 경계를 만듦 → 자연스러움 <br>


**Hyperparameter**

K값과 거리척도는 **Hyperparameter**로 학습 전 사전에 반드시 선택해야 함

1. 학습 데이터의 정확도와 성능을 최대화하는 Hyperparameter를 선택 → **BAD**

  <img width="1059" alt="L2_07" src="https://user-images.githubusercontent.com/86525868/170635130-2012f25d-1d85-49c5-a292-7ad56f918de4.png">{: width="80%" height="80%"}{: .align-center}

  * **K=1**일 때 가장 학습 데이터의 정확도가 높지만 **K를 큰 값**으로 선택하는 것이 학습 데이터에서는 몇 개 잘못 분류할 수 있지만 테스트 데이터에서는 더 좋은 성능을 보일 수 있음    
  * **학습 데이터를 얼마나 잘 맞추는지는 중요하지 않음, 새로운 데이터에 좋은 성능을 보이는 것이 중요

   

2. 전체 데이터 셋 중 학습 데이터를 쪼개서 일부를 테스트 데이터로 사용 (학습 데이터로 다양한 Hyperparameter값들을 학습시키고 테스트 데이터에서 가장 좋은 성능을 보이는 Hyperparameter 선택) → **BAD**

  <img width="1090" alt="L2_08" src="https://user-images.githubusercontent.com/86525868/170635138-ae08111e-67c5-4667-b7cc-97ce0c29eb32.png">{: width="80%" height="80%"}{: .align-center}

  * 학습시킨 모델들 중 테스트 데이터에 가장 잘 맞는 모델을 선택하는 것은 테스트 셋에서만 잘 작동하는 Hyperparameter를 고른 것일 수 있음

3. 데이터를 Training set, Vaildation set, Testing set으로 나눔

  <img width="1034" alt="L2_09" src="https://user-images.githubusercontent.com/86525868/170635141-5b18e41b-efa0-4cdd-832e-357e6fd98b9f.png">{: width="80%" height="80%"}{: .align-center}

  * 다양한 Hyperparameter로 트레이닝 셋을 학습 → 벨리데이션 셋으로 검증 → 테스트 셋에서 "**한번만**" 수행

4. Cross Validation (5-fold cross validation)

  <img width="1009" alt="L2_10" src="https://user-images.githubusercontent.com/86525868/170635144-4a476e33-8bc1-40e1-b214-4db2ab7ce0e5.png">{: width="80%" height="80%"}{: .align-center}

  * 테스트 데이터를 제외한 나머지 데이터를 여러 부분으로 나누고 번갈아가면서 벨리데이션 셋을 지정 
  * 여러번의 검증을 거치면서 최적의 하이퍼파리미터를 확인할 수 있음 <br>

   

**But K-Nearest Neighbor on images never used.**

1. Very slow at test time

2. Distance metrics on pixels are nor informative 

  <img width="1054" alt="L2_11" src="https://user-images.githubusercontent.com/86525868/170635145-6059aa38-f399-41fb-9acc-d6eabdf10a4a.png">{: width="80%" height="80%"}{: .align-center}

   각 이미지와 원본 사이의 Euclidean Distance를 측정하면 동일한 L2 Distance를 가지는 것을 알 수 있음 → L2 Distance는 "지각적 유사도"를 측정하기에 적합하지 않음

3. Curse of Dimensionality 

   k-nn은 트레이닝 데이터로 공간을 분할하는 일을 수행 → 잘 동작하려면 전체 공간을 조밀하게 커버할 만큼의 충분한 트레이닝 샘플이 필요함

   공간을 조밀하게 덮으려면 차원이 증가함에 따라 데이터의 양의 기하급수적으로 증가함 <br>



## Second Classifier : Linear Classifier 

Linear Classifier은 **Parametric model**의 가장 단순한 형태

1. **x** : 입력 이미지 
2. **w(theta)** : 가중치 
3. **b(Bias)** : 편향, 데이터와 무관하게 특정 클래스에 우선권 부여

트레이닝 데이터의 정보를 요약해 **W**에 모으는 방식을 사용 → 효율적임

딥러닝은 가중치(**W**)와 데이터를 어떻게 조합할지 함수를 적절하게 설계하는 일로 설명할 수 있음

<img width="1117" alt="L2_12" src="https://user-images.githubusercontent.com/86525868/170635151-13324cc7-ddf4-44b0-b1bd-800d28244601.png">{: width="80%" height="80%"}{: .align-center}

$F(x, w) = Wx$ →  Linear Classification 

입력 이미지와 가중치를 가지가 함수 $f$를 통해 10개의 class scoresf를 결과로 내놓고 이것 중 클래스의 점수가 가장 높은 클래스로 분류됨

<img width="1135" alt="L2_13" src="https://user-images.githubusercontent.com/86525868/170635153-1e16c05a-4e4c-4fe8-a3ea-46aca361f278.png">{: width="80%" height="80%"}{: .align-center}

<br>

**Interpreting Linear Classification**

<img width="1110" alt="L2_14" src="https://user-images.githubusercontent.com/86525868/170635156-d52a563a-182b-4f20-99d4-a920b7f81bf9.png">{: width="80%" height="80%"}{: .align-center}

각 이미지를 고차원 공간의 한 점이라고 생각하면 Linear Classifier은 각 클래스를 구분시켜 주는 선형 결정 경계를 그어주는 역할을 함

하지만, 이미지가 고차원 공간의 하나의 점이라는 관점으로 해석하면 직면하는 문제가 있음

<img width="1116" alt="L2_15" src="https://user-images.githubusercontent.com/86525868/170635158-8631ed1a-c002-4c91-b04d-259ca9fd7fd8.png">{: width="80%" height="80%"}{: .align-center}

데이터를 선 하나로 분류할 수 있는 방법이 없는 경우 → **홀/짝수를 분류하는 반전성 문제(parity probem)** 해결에 어려움



