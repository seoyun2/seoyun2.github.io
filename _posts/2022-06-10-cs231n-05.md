---
title: "[CS231N] 6. Training Neural Network 1"
categories:
  - cs231n
  - Image Classification
tags:
  - cs231
  - image classification
toc: true
toc_sticky: true
toc_label: "On This Page"
toc_icon: "bookmark"
use_math: true
---


💡 Iamge Classification <br>
Stanford University "CS231N" 강의를 듣고 정리하였습니다. <br>
{: .notice--info}


자막 : https://github.com/visionNoob/CS231N_17_KOR_SUB (크롬 확장 프로그램 Subtitles for Youtube) <br>
2017 버전 강의 목차 및 슬라이드 : http://cs231n.stanford.edu/2017/syllabus.html <br>
2017 버전 강의 동영상 링크 : https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv <br>
2022 버전 강의 목차 및 슬라이드 : http://cs231n.stanford.edu/schedule.html <br>
실습 과제 : https://github.com/cs231n/cs231n.github.io/tree/master/assignments <br>
{: .notice}

# Lectrue 6 : Training Neural Networks 1

## Activation Function 

데이터의 입력이 들어오면 가중치와 곱하고 활성화함수, 즉 비선형 연산을 거치게 됨 

<img width="760" alt="L6_01" src="https://user-images.githubusercontent.com/86525868/172896345-9f917032-5c7f-4e87-89e9-1fcc1d861f35.png">{: width="50%" height="50%"}{: .align-center}



### Sigmoid

<img width="341" alt="L6_02" src="https://user-images.githubusercontent.com/86525868/172896376-f329f399-8b0b-4a06-a397-478657230f97.png">{: width="20%" height="20%"}{: .align-center}


$$
\sigma(x)=1/(1-e^{-x})
$$

각 입력을 받아서 그 입력을 [0, 1] 사이의 값이 되도록 함 (입력값이 크면 Sigmoid의 출력은 1에 가깝고 값이 작으면 0에 가까움)

Sigmoid가 뉴런의 firing rate를 saturation 시키는 것으로 해석할 수 있기 때문에 많이 사용됨 (어떤 값이 0에서 1 사이의 값을 가지면 이를 firing rate라고 생각할 수 있음)



#### Sigmoid의 3가지 문제점

1. __Saturated neurons "kill" the gradients__

    <img width="1133" alt="L6_03" src="https://user-images.githubusercontent.com/86525868/172896138-d8ad9cb3-3d89-4610-baad-794ae5a7d5bc.png">{: width="50%" height="50%"}{: .align-center}

    입력 x가 있고 출력이 있다면 Backprop에서 Gradient는 기울기가 구해지는 과정에서 연쇄법칙으로 Backprop(오차역전파)됨 

    Sigmoid에서 음의 큰 값(x = -10)이면 Sigmoid가 flat하게 되고 gradient가 거의 0에 가까운 값이 backprop됨 

    __이 부분에서 gradient가 죽어버리게 되고 밑으로 0이 계속 전달되게 됨__. 양의 큰 값일 경우도 마찬가지 

   

2. __Sigmoid output are not zero-centered__

    <img width="982" alt="L6_04" src="https://user-images.githubusercontent.com/86525868/172896142-f378cb52-53e2-4a31-8b1a-c32e807cb17c.png">{: width="50%" height="50%"}{: .align-center}


    뉴런의 입력(x)이 항상 양수일 때 x는 어떤 가중치랑 곱해지고 활성화 함수를 통과함

    <img width="409" alt="L6_05" src="https://user-images.githubusercontent.com/86525868/172896156-fe3071bd-8c18-41ef-b9cc-1ac6426abf28.png">{: width="50%" height="50%"}{: .align-center}


    이 때 gradient는 __"전부 양수" 또는 "전부 음수"__가 됨
  
    위에서 dL/df(활성화함수)가 음수 또는 양수로 넘어오고 local gradient는 이 값이랑 곱해지고 df(활성화함수)/dW는 그냥 x가 됨. 이렇게 되면 gradient의 부호는 위에서 내려온 gradient의 부호와 같아짐. 이는, __W가 모두 같은 방향으로만 움직이게 되는 것을 의미__. (파라미터를 업데이트 할 때 다 같이 증가하거나 다 같이 감소함) 이런 gradient업데이트는 아주 __비효율적__인 방법. 

    그림처럼 파란색 화살표 방향으로 내려가지 못하고 빨간색 화살표처럼 내려가기 때문에 비효율적

    그렇기 때문에 __"zero-mean data"__를 원함 (x가 양수와 음수를 모두 가지고 있으면 전부 같은 뱡향으로 움직이는 일은 발생하지 않음)



3. exp() is a bit compute expensive (exp()로 인해 계산비용이 큼) -> 다른 내적 계산 비용이 더 크기 때문에 큰 문제는 아님



### tanh(x)

<img width="343" alt="L6_06" src="https://user-images.githubusercontent.com/86525868/172896158-f18eaf2b-ad65-4711-8f73-4cd34ad15d72.png">{: width="20%" height="20%"}{: .align-center}


Sigmoid와 유사하지만 범위가 [-1, 1]임. 가장 큰 차이점은 __zero-centered__ 이기 때문에 Sigmoid 함수의 두번째 문제는 해결이 되지만, saturation 때문에 여전히 gradient는 죽음



### ReLU

<img width="392" alt="L6_07" src="https://user-images.githubusercontent.com/86525868/172896160-80d02872-a0c8-486f-8854-8c037bc9f237.png">{: width="20%" height="20%"}{: .align-center}


$$
f(X)=max(0, 1)
$$

ReLU 함수는 element-wise 연산을 수행하며 입력이 음수면 값이 0이 되고 양수면 입력 값 그대로를 출력함

양의 값에서는 saturation되지 않기 때문에 적어도 입력 스페이스의 절반은 saturation 되지 않음 (__ReLU의 가장 큰 장점__)

그리고 계산 효율이 뛰어남. 기존의 Sigmoid함수는 함수 안에 지수항이 있었지만 ReLU는 단순한 max 연산이므로 계산이 매우 빠름.(실제로 Sigmoid, tanh보다 수렴속도가 6배 정도 빠름) 실제 신경과학적인 실험을 통해 뉴런을 관찰해보면 Sigmoid보다 ReLU스러운것을 알 수 있음

__ReLU의 문제점__

1. __Not zero-centered output__

   tanh가 이 문제는 해결했지만 ReLU는 여전히 문제를 가지고 있음

   

2. __양의 수에서는 saturation 되지만, 음의 경우엔 그렇지 않음__

   <img width="1091" alt="L6_08" src="https://user-images.githubusercontent.com/86525868/172897047-5780b55b-39e9-4c8e-9d62-1e516b3d1c5c.png">{: width="50%" height="50%"}{: .align-center}


3. __몇가지 상황에서 DEAD ReLU 발생__

   <img width="1096" alt="L6_09" src="https://user-images.githubusercontent.com/86525868/172896168-1ef8e57f-879a-4f28-8cf5-9314d5395ca4.png">{: width="50%" height="50%"}{: .align-center}


  ReLU에서는 평면의 절반만 activate 된다는 것을 알 수 있음 (각 평면(초록 빨강 직선)이 각 ReLU를 의미한다고 생각하기). ReLU가 data cloud에서 떨어져 있는 경우에 "dead ReLU"가 발생함 (dead ReLU에서는 activate가 일어나지 않고 update되지 않음). 반면에, active ReLU에서는 일부는 active되고, 일부는 active하지 않음

   * 초기화를 잘못한 경우 (가중치 평면이  data cloud에서 멀리 떨어져 있는 경우) → 어떤 데이터 입력에섬도 activate 되는 경우가 존재하지 않고 backprop이 일어나지 않음

   * Learning rate가 지나치게 높은 경우 → 적절한 ReLU로 시작할 수 있다고 해도 update를 지나치게 크게 해 가중치가 날뛴다면 ReLU가 데이터의 manifold를 벗어나게 됨

   그렇기 때문에 __ReLU를 초기화할 때, positive biases를 추가하거나 zero-bias로 초기화함__(update시 active ReLU가 될 가능성을 높이기 위해)



#### Leaky ReLU

<img width="370" alt="L6_10" src="https://user-images.githubusercontent.com/86525868/172896171-913eb7e6-79e1-4259-85b1-cf34a5478071.png">{: width="20%" height="20%"}{: .align-center}


$$
f(x)=max(0.01x, x)
$$

ReLU 와 유사하지만 negative regime에서 더이상 0이 아님.. negative에도 기울기를 살짝 주면서 앞선 문제들를 상당 부분 해결하고 계산도 효율적임 



#### PReLU(parametric rectifier)

$$
f(x)=max(\alpha x, x)
$$

negative space에 기울기가 있다는 점에서 Leaky ReLU와 유사하지만 기울기가 alpha라는 파라미터로 결정됨 (alpha를 딱 정해놓는 것이 아니라 backprop으로 학습시키는 파라미터로 만들어 활성함수가 조금 더 유연해짐)



#### ELU(Exponential Linear Units)

<img width="302" alt="L6_11" src="https://user-images.githubusercontent.com/86525868/172896173-1f846be5-2f61-4f0a-a8db-2a31c11a2fec.png">{: width="20%" height="20%"}{: .align-center}


$$
f(x)=
\begin{cases}
x, & \mbox{if }x>0 \\
\alpha(e^x-1), & \mbox{if }x\le0
\end{cases}
$$


ReLU의 이점을 모두 가져오지만 zero-mean에 가까운 출력값을 보임 

