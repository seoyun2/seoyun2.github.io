var store = [{
        "title": "[Time Series] 시계열 데이터",
        "excerpt":"💡 시계열 데이터 분석 기초 유튜브 “곽기영” 강의를 보고 정리하였습니다. 시계열 데이터 시계열 분석 개요 시계열분석(time series analysis)은 시간의 흐름에 따라 일정한 간격으로 사건을 관찰하여 기록한 데이터(시계열 데이터)를 바탕으로 미래의 관측값을 예측(forecasting)하는 분석 기법이다. 과거의 일련의 관측값을 분석하여 이를 모델링하고, 이 예측모델을 바탕으로 미래의 관측값을 예측한다. 시계열 데이터는 일반적으로 추세...","categories": ["TimeSeries"],
        "tags": ["time series","statistics"],
        "url": "http://localhost:4000/timeseries/time-series-01/",
        "teaser": null
      },{
        "title": "[Hands On ML] 한눈에 보는 머신러닝",
        "excerpt":"💡 Hands On Machine Learning 오렐리앙 “핸즈온 머신러닝”을 보고 정리하였습니다. 한눈에 보는 머신러닝 머신러닝 시스템의 종류 지도학습과 비지도 학습 지도학습(supervised learning) : 정답이 훈련데이터에 포함되어 있음 분류(classification) - 스팸 필터 회귀(regression) - 중고차 가격 예측 (특성을 나타내는 예측변수를 사용해 가격이라는 타겟수치 예측) 비지도 학습(unsupervised learning) : 정답이 훈련데이터에 포함되어 있지...","categories": ["HandsOnML"],
        "tags": ["machineLearning","statistics"],
        "url": "http://localhost:4000/handsonml/HandsOnML-01/",
        "teaser": null
      },{
        "title": "[MySQL] MySQL 정리 1",
        "excerpt":"MySQL 소개 MySQL은 가장 널리 사용되고 있는 관계형 데이터베이스 관리 시스템 (RDBMS:Realational DBMS) MySQL은 오픈 소스이며, 다중 사용자와 다중 스레드를 지원 C언어, C++, JAVA, PHP 등 여러 프로그래밍 언어를 위한 다양한 API를 제공 MySQL은 유닉스, 리눅스, 윈도우 등 다양한 운영체제에서 사용할 수 있으며, 특히 PHP와 함께 웹 개발에 자주 사용...","categories": ["MySQL"],
        "tags": ["MySQL"],
        "url": "http://localhost:4000/mysql/MySQL-01/",
        "teaser": null
      },{
        "title": "[MySQL] MySQL 정리 2",
        "excerpt":"MySQL 내장함수 사람들의 편의를 위해 다양한 기능의 내장 함수를 미리 정의하여 제공 대표적인 내장함수 종류 문자열 함수 수학 함수 날짜와 시간 함수 LENGTH() 전달받은 문자열의 길이를 반환 SELECT LENGTH('abcdefg'); &gt;&gt;&gt; 7 CONCAT() 전달받은 문자열을 모두 결합하여 하나의 문자열로 반환 전달받은 문자열 중 하나라도 NULL이 존재하면 NULL을 반환 SELECT CONCAT('My', 'SQL...","categories": ["MySQL"],
        "tags": ["MySQL"],
        "url": "http://localhost:4000/mysql/MySQL-02/",
        "teaser": null
      },{
        "title": "[CS231N] 2. Image Classification",
        "excerpt":"💡 Iamge Classification Stanford University “CS231N” 강의를 듣고 정리하였습니다. 자막 : https://github.com/visionNoob/CS231N_17_KOR_SUB (크롬 확장 프로그램 Subtitles for Youtube) 2017 버전 강의 목차 및 슬라이드 : http://cs231n.stanford.edu/2017/syllabus.html 2017 버전 강의 동영상 링크 : https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv 2022 버전 강의 목차 및 슬라이드 : http://cs231n.stanford.edu/schedule.html 실습 과제 : https://github.com/cs231n/cs231n.github.io/tree/master/assignments 2. Image Classification 컴퓨터에게 고양이...","categories": ["cs231n"],
        "tags": ["cs231","image classification"],
        "url": "http://localhost:4000/cs231n/cs231n-01/",
        "teaser": null
      },{
        "title": "[CS231N] 3. Loss Functions and Optimization",
        "excerpt":"💡 Iamge Classification Stanford University “CS231N” 강의를 듣고 정리하였습니다. 자막 : https://github.com/visionNoob/CS231N_17_KOR_SUB (크롬 확장 프로그램 Subtitles for Youtube) 2017 버전 강의 목차 및 슬라이드 : http://cs231n.stanford.edu/2017/syllabus.html 2017 버전 강의 동영상 링크 : https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv 2022 버전 강의 목차 및 슬라이드 : http://cs231n.stanford.edu/schedule.html 실습 과제 : https://github.com/cs231n/cs231n.github.io/tree/master/assignments Lecture 3 : Loss Functions...","categories": ["cs231n"],
        "tags": ["cs231","image classification"],
        "url": "http://localhost:4000/cs231n/cs231n-02/",
        "teaser": null
      },{
        "title": "[CS231N] 4. Intruduction to Neural Networks",
        "excerpt":"💡 Iamge Classification Stanford University “CS231N” 강의를 듣고 정리하였습니다. 자막 : https://github.com/visionNoob/CS231N_17_KOR_SUB (크롬 확장 프로그램 Subtitles for Youtube) 2017 버전 강의 목차 및 슬라이드 : http://cs231n.stanford.edu/2017/syllabus.html 2017 버전 강의 동영상 링크 : https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv 2022 버전 강의 목차 및 슬라이드 : http://cs231n.stanford.edu/schedule.html 실습 과제 : https://github.com/cs231n/cs231n.github.io/tree/master/assignments Lecture 4 : Intruduction to...","categories": ["cs231n"],
        "tags": ["cs231","image classification"],
        "url": "http://localhost:4000/cs231n/cs231n-03/",
        "teaser": null
      },{
        "title": "[CS231N] 5. Convolutional Neural Networks",
        "excerpt":"💡 Iamge Classification Stanford University “CS231N” 강의를 듣고 정리하였습니다. 자막 : https://github.com/visionNoob/CS231N_17_KOR_SUB (크롬 확장 프로그램 Subtitles for Youtube) 2017 버전 강의 목차 및 슬라이드 : http://cs231n.stanford.edu/2017/syllabus.html 2017 버전 강의 동영상 링크 : https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv 2022 버전 강의 목차 및 슬라이드 : http://cs231n.stanford.edu/schedule.html 실습 과제 : https://github.com/cs231n/cs231n.github.io/tree/master/assignments Lecture 5 : Convolutional Neural...","categories": ["cs231n"],
        "tags": ["cs231","image classification"],
        "url": "http://localhost:4000/cs231n/cs231n-04/",
        "teaser": null
      },{
        "title": "[CS231N] 6. Training Neural Network 1",
        "excerpt":"💡 Iamge Classification Stanford University “CS231N” 강의를 듣고 정리하였습니다. 자막 : https://github.com/visionNoob/CS231N_17_KOR_SUB (크롬 확장 프로그램 Subtitles for Youtube) 2017 버전 강의 목차 및 슬라이드 : http://cs231n.stanford.edu/2017/syllabus.html 2017 버전 강의 동영상 링크 : https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv 2022 버전 강의 목차 및 슬라이드 : http://cs231n.stanford.edu/schedule.html 실습 과제 : https://github.com/cs231n/cs231n.github.io/tree/master/assignments Lectrue 6 : Training Neural...","categories": ["cs231n"],
        "tags": ["cs231","image classification"],
        "url": "http://localhost:4000/cs231n/cs231n-05/",
        "teaser": null
      },{
        "title": "[Data Analysis] 1. Multiple Linear Regression Analysis",
        "excerpt":"다중 선형 회귀 모형 : Multivariate Linear Regression 목적 : 종속변수 $Y$와 설명변수 집합 $x_1, x_2, \\cdots, x_p$ 사이의 관계를 선형으로 가정하고 이를 가장 잘 설명할 수 있는 회귀 계수(Regression coefficients)를 추정 \\[y=\\beta_0+\\beta_1x_1+\\cdots+\\beta_dx_d+\\epsilon \\ \\ \\ \\ \\Longrightarrow \\ \\ \\ \\ \\hat{y}=\\hat{\\beta_0}+\\hat{\\beta_1}x_1+\\cdots+\\hat{\\beta_dx}_d\\] 우리가 알고있는 $y, x_1, x_2, \\cdots, x_d$...","categories": ["da"],
        "tags": ["Data Analysis","Linear Regression"],
        "url": "http://localhost:4000/da/analysis-01/",
        "teaser": null
      },{
        "title": "[Data Analysis] 2. Logistic Regression",
        "excerpt":"로지스틱 회귀분석 : Logisitc Linear Regression 💡 Data business 강필성 교수님의 강의를 보고 정리하였습니다. 다중 선형 회귀분석 : 수치형 설명변수 $X$와 종속변수 $Y$ 간의 관계를 선형으로 가정하고 이를 잘 표현할 수 있는 회귀계수를 추정 $y$ 와 $\\hat{y}$ 의 차이가 적게 되는 $\\hat{\\beta_i}=(X^T X)^{-1}X^TY$ 를 추정 예시 33명의 성인 여성에 대한...","categories": ["da"],
        "tags": ["Data Analysis","Logisitc Regression"],
        "url": "http://localhost:4000/da/LogisticRegression-01/",
        "teaser": null
      },{
        "title": "[Data Analysis] 3. Confusion Matrix",
        "excerpt":"Chapter 3. 분류 모형 성능 평가 💡 Data business 강필성 교수님의 강의를 보고 정리하였습니다. 학습 데이터에 대해서 100% 정확한 모형을 만들면 좋은 것인가? Training data에 대해서 빨간색 경계면은 완벽하게 분류하고 있지만 파란색 경계면은 다소 오분류된 것을 볼 수 있음 → 빨간 경계면이 파란 경계면 보다 좋은가? NO! 모델이 학습 데이터에...","categories": ["da"],
        "tags": ["Data Analysis","Confusion Matrix"],
        "url": "http://localhost:4000/da/auroc/",
        "teaser": null
      },{
        "title": "[Data Analysis] 4. Decision Tree",
        "excerpt":"Chapter 4. 의사결정나무 💡 Data business 강필성 교수님의 강의를 보고 정리하였습니다. 여러 가지 머신러닝 알고리즘이 필요한 이유? → 특정 알고리즘이 모든 상황에서 다른 알고리즘보다 우월하다는 결론을 내릴 수 없음 목적 한번에 하나씩의 설명변수를 사용하여 정확한 예측(+설명)이 가능한 규칙들의 집합을 생성 최종 결과물은 나무를 뒤집어 높은 형태인 규칙들의 집합 예시 :...","categories": ["da"],
        "tags": ["Data Analysis","Decision Tree"],
        "url": "http://localhost:4000/da/decisiontree/",
        "teaser": null
      },{
        "title": "[Data Analysis] 5. Variable Selection",
        "excerpt":"Chapter 5. 주요 변수 선택 기법 💡 Data business 강필성 교수님의 강의를 보고 정리하였습니다. 차원축소 : Dimensionality Reduction 차원의 저주 : Curse of Dimensionality 동등한 설명력을 갖기 위해서는 변수가 증가할 때 필요한 개체의 수는 기하급수적으로 증가 차원의 저주 (Curse of Dimensionality) : 데이터의 차원이 높아짐에 따라 생기는 문제점 변수가 증가할수록...","categories": ["da"],
        "tags": ["Data Analysis","Variable Selection","Exhaustive Serach","Forward Selection","Backward Elimination","Stepwise Selection","Genetic algorithm"],
        "url": "http://localhost:4000/da/variable-selection/",
        "teaser": null
      },{
        "title": "[Data Analysis] 6. K-Nearest Neighbor Learning",
        "excerpt":"Chapter 6. k-인접 이웃 기법 💡 Data business 강필성 교수님의 강의를 보고 정리하였습니다. K-인접 이웃 기법 : K-Nearest Neighbor Learning Model-based Learning 학습데이터로부터 설명변수 X와 종속변수 Y의 관계를 찾아내는 모델을 학습한 뒤, 새로운 객체의 설명변수 X를 학습된 모델에 투입하여 Y를 예측하는 방식 (Linear Regression, Logistic Regreesion, Decision Tree, Neural Network,...","categories": ["da"],
        "tags": ["Data Analysis","K-Nearest Neighbor Learning","K-Nearest Neighbor Regression","K-Nearest Neighbor Classification"],
        "url": "http://localhost:4000/da/K-nearest-regression/",
        "teaser": null
      },{
        "title": "[R] 1. Multiple Linear Regression",
        "excerpt":"Multiple Linear Regression Practice with R Functions # Performance evaluation function for regression perf_eval_reg &lt;- function(tgt_y, pre_y){ #RMSE rmse &lt;- sqrt(mean((tgt_y - pre_y)^2)) #MAE mae &lt;- mean(abs(tgt_y - pre_y)) #MAPE mape &lt;- 100* mean(abs((tgt_y - pre_y)/tgt_y)) return(c(rmse, mae, mape)) } # Initialize a performance summary perf_mat &lt;- matrix(0, nrow =...","categories": ["R","MultipleLinearRegression"],
        "tags": ["Multiple Linear Regression","R"],
        "url": "http://localhost:4000/r/multiplelinearregression/r-mlr/",
        "teaser": null
      },{
        "title": "[R] 2. Logistic Regression with Confusion Matrix",
        "excerpt":"Logistic Regression Practice with R Performance Evaluation Function 1 perf_eval1 &lt;- function(cm){ # True positive rate : TPR, recall TPR &lt;- cm[2, 2]/sum(cm[2, ]) # Precision PRE &lt;- cm[2, 2]/sum(cm[, 2]) # True negative rate : TNR TNR &lt;- cm[1, 1]/sum(cm[1, ]) # Accuracy ACC &lt;- (cm[1, 1]+cm[2, 2])/sum(cm) #...","categories": ["R","LogisticRegression","ConfusionMatrix"],
        "tags": ["Logistic Regression","Confusion Matrix","R"],
        "url": "http://localhost:4000/r/logisticregression/confusionmatrix/auroc-r/",
        "teaser": null
      },{
        "title": "[R] 3. Decision Tree with Confusion Matrix",
        "excerpt":"Decision Tree Practice with R Performance Evaluation Function 1 perf_eval &lt;- function(cm){ # True positive rate: TPR (Recall) TPR &lt;- cm[2,2]/sum(cm[2,]) # Precision PRE &lt;- cm[2,2]/sum(cm[,2]) # True negative rate: TNR TNR &lt;- cm[1,1]/sum(cm[1,]) # Simple Accuracy ACC &lt;- (cm[1,1]+cm[2,2])/sum(cm) # Balanced Correction Rate BCR &lt;- sqrt(TPR*TNR) # F1-Measure F1...","categories": ["R","DecisionTree","CART"],
        "tags": ["Decision Tree","R"],
        "url": "http://localhost:4000/r/decisiontree/cart/decision-tree-r/",
        "teaser": null
      },{
        "title": "[Data Science] 1. 커널 기반 학습(Kernel Based Learning)",
        "excerpt":"1. Kernel Based Learning 💡 Data Science 강필성 교수님의 강의를 보고 정리하였습니다. 이론적 배경 SHATTER 함수 $F$는 n개의 point를 shatter할 수 있다 → 함수 $F$에 의해 n개의 point는 임의의 +1, -1을 Target value로 하는 분류 경계면의 생성(Binominal Classification)이 가능하다. ↳ 예시에서는 점이 3개 이므로 생성 가능한 경계면은 8가지 ($2^3$) 선형분류기는...","categories": ["ds"],
        "tags": ["Data Analysis","Kernel Based Learning"],
        "url": "http://localhost:4000/ds/kernel-based-learning1/",
        "teaser": null
      },{
        "title": "[Data Science] 2. 이상치 탐지(Density Based Anomaly Detection)",
        "excerpt":"2. Density Based Anomaly Detection 💡 Data Science 강필성 교수님의 강의를 보고 정리하였습니다. 이론적 배경 이상치 데이터란 이상치 데이터는 노이즈 데이터와 다름 (노이즈는 어찌할 수 없는 데이터로 측정 과정에서의 무작위성에 기반) 하지만, 이상치 데이터는 정상적인 데이터를 생성하는 매커니즘을 위반하여 생성되는 데이터로 적지만 중요한 데이터 이상치 탐지 사례 Preventive Maintenance 충분하게...","categories": ["ds"],
        "tags": ["Data Analysis","Anomaly Detection"],
        "url": "http://localhost:4000/ds/density-based-anomaly-detection/",
        "teaser": null
      },{
        "title": "[Data Science] 앙상블(Ensemble) 1. 배깅(Bagging), 랜덤 포레스트(Random Forest)",
        "excerpt":"Ensemble “No Free Lunch Theorem” ‘No Free Lunch Theorem’은 간단히 말해 특정한 문제에 최적화된 알고리즘은 다른 문제에서는 그렇지 않다는 것을 수학적으로 증명한 정리이다. 성공으로 가는 지름길은 없고 어떤 알고리즘도 모든 상황에서 우월할 수 는 없다는 것을 의미한다. 즉, 우리는 문제의 목적, 데이터 형태 등을 종합적으로 고려하여 최적의 알고리즘을 선택할 필요가...","categories": ["ds"],
        "tags": ["Data Analysis","Ensemble"],
        "url": "http://localhost:4000/ds/ensemble-1/",
        "teaser": null
      },{
        "title": "[Data Science] 앙상블(Ensemble) 2. AdaBoost, GBM",
        "excerpt":"Ensemble AdaBoost Weak Model vs Strong Model Strong model은 계산 복잡도가 높은 모델이나 하이퍼파라미터가 많은 모델로 복잡한 분류 경계면이나 복잡한 회귀직선을 추정해 내는 모델들을 의미한다. Weak model은 랜덤 모델(승률이 50:50인 모델)d에 비해 약간의 성능 개선이 있는 모델을 의미한다. Weak model은 Boosting의 일련의 절차를 통해서 성능이 우수한 모델로 향상될 수 있다....","categories": ["ds"],
        "tags": ["DataScience","Ensemble","Boosting","Adaboost","GBM"],
        "url": "http://localhost:4000/ds/ensemble-2/",
        "teaser": null
      },{
        "title": "[Data Science] 앙상블(Ensemble) 3. XGBoost, LightGBM",
        "excerpt":"Ensemble XGBoost 단일 의사 결정나무가 가지는 과적합 위험을 방지하기 위해 복원추출이 핵심인 bootstrap을 만드는 Bagging이 제안되었다. Bagging은 복원추출을 함으로써 앙상블의 다양성은 확보한다. 그 다양성 확보의 극대화를 위해서 tree를 split할 때마다 모든 변수를 사용하는 것이 아닌 일부분의 변수만 후보로 사용하는 Random Forest 기법이 제안되었다. Bagging은 처음부터 데이터를 복원 추출하여 여러개로 생성하면서...","categories": ["ds"],
        "tags": ["DataScience","Ensemble","Boosting","XGBoost","LightGBM"],
        "url": "http://localhost:4000/ds/ensemble-3/",
        "teaser": null
      },{
        "title": "[SQL Hackerrank] Easy",
        "excerpt":"Q1. Query all columns for all American cities in the CITY table with populations larger than 100000. The CountryCode for America is USA. SELECT * FROM CITY WHERE POPULATION &gt; 100000 Q2. Query the NAME field for all American cities in the CITY table with populations larger than 120000. The...","categories": ["hackerrank"],
        "tags": ["sql","haker rank","coding test"],
        "url": "http://localhost:4000/hackerrank/sql-codingtest-1/",
        "teaser": null
      },{
        "title": "[Kaggle] Porti Seguro's Safe Driver Prediction",
        "excerpt":"Porti Seguro’s Safe Driver Prediction Porti Segure’s Sage Driver Prediction 데이터 설명 유사한 그룹에 속하는 특징들은 특징 이름에 해당하는 태그로 표시됩니다 (예 : ind, reg, car, calc). 이진 특징은 postfix “bin”으로 표시되며 범주형 특징은 “cat”으로 표시됩니다. 이러한 표기가 없는 특징은 연속형 또는 서수형입니다. 값 -1은 관찰에서 특징이 누락되었음을 나타냅니다. 타겟...","categories": ["kaggle"],
        "tags": ["kaggle"],
        "url": "http://localhost:4000/kaggle/porti-kaggle/",
        "teaser": null
      }]
